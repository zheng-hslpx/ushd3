import os
# 8.21ä¿®æ”¹_åºå·1ï¼šè®¾ç½®éäº¤äº’å¼åç«¯ï¼Œç¡®ä¿æ— æ˜¾ç¤ºç¯å¢ƒä¹Ÿèƒ½ä½œå›¾
# è¯´æ˜ï¼šåœ¨æœåŠ¡å™¨/æ— GUIç¯å¢ƒè®­ç»ƒæ—¶ï¼Œä½¿ç”¨Aggåç«¯é¿å…matplotlibæŠ¥é”™ï¼›plt.ioffå…³é—­äº¤äº’æ¨¡å¼é‡Šæ”¾å†…å­˜ã€‚
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
plt.ioff()

import json, argparse, csv, re
from pathlib import Path
from datetime import datetime
from typing import Dict
import numpy as np
import torch
from tqdm import tqdm

# 8.21ä¿®æ”¹_åºå·2ï¼šè¡¥å……æ ‡å‡†åº“å¯¼å…¥ï¼ˆé¢„ç•™ï¼‰
# è¯´æ˜ï¼šå½“å‰ç‰ˆæœ¬æœªç›´æ¥ä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥ä¾¿åç»­åœ¨è®­ç»ƒ/è¯„ä¼°é˜¶æ®µåšå®‰å…¨å‰¯æœ¬ã€‚
from copy import deepcopy
from typing import Optional

from usv_agent.usv_env import USVEnv
from usv_agent.hgnn_model import HeterogeneousGNN
from usv_agent.ppo_policy import PPOAgent, PPO, Memory
from utils.vis_manager import VisualizationManager

# è®¾ç½®å…¨å±€é»˜è®¤æ•°æ®ç±»å‹
torch.set_default_dtype(torch.float32)


def load_config(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def _device_from_cfg(model_cfg):
    want = str(model_cfg.get('device', 'auto')).lower()
    if want == 'auto':
        return torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if want.startswith('cuda') and not torch.cuda.is_available():
        print("[WARN] CUDA is not available, falling back to CPU")
        return torch.device('cpu')
    return torch.device(want)


def _group_name(cfg):
    return cfg['train_paras'].get('run_name', 'default_run')


def _next_run_index(dir_path: Path) -> str:
    dir_path.mkdir(parents=True, exist_ok=True)
    nums = [int(p.name) for p in dir_path.iterdir() if p.is_dir() and re.fullmatch(r'\d{2,}', p.name)]
    return f"{(max(nums) + 1) if nums else 1:02d}"


def _short_ts():
    return datetime.now().strftime("%m%d_%H%M%S")


def compute_lookahead_features(state: Dict, device: torch.device) -> torch.Tensor:
    """
    è¯´æ˜ï¼šåŸºäºå½“å‰çŠ¶æ€æ„é€ USV-Taskè¾¹ç‰¹å¾ï¼ŒåŒ…å«å½’ä¸€åŒ–è·ç¦»ã€ä»»åŠ¡é‚»è¿‘åº¦ã€USVæœºä¼šåº¦ã€‚
    """
    usv_pos = torch.from_numpy(state['usv_features'][:, :2]).float().to(device)
    task_pos = torch.from_numpy(state['task_features'][:, :2]).float().to(device)
    task_status = torch.from_numpy(state['task_features'][:, 3]).float().to(device)
    map_size = torch.tensor(state['map_size'], dtype=torch.float32, device=device)

    U, T = usv_pos.shape[0], task_pos.shape[0]
    dist_ut = torch.cdist(usv_pos, task_pos)

    unassigned_mask = task_status > 0
    unassigned_pos = task_pos[unassigned_mask]

    feat_task_proximity = torch.zeros(U, T, device=device)
    if unassigned_pos.shape[0] > 1:
        dist_tt_unassigned = torch.cdist(unassigned_pos, unassigned_pos)
        dist_tt_unassigned.fill_diagonal_(float('inf'))
        min_dist_tt, _ = torch.min(dist_tt_unassigned, dim=1)
        temp_prox = torch.zeros(T, device=device)
        temp_prox[unassigned_mask] = min_dist_tt
        feat_task_proximity = temp_prox.unsqueeze(0).expand(U, -1).clone()

    feat_usv_opportunity = torch.zeros(U, T, device=device)
    if unassigned_pos.shape[0] > 0:
        dist_usv_to_unassigned = torch.cdist(usv_pos, unassigned_pos)
        min_dist_usv, _ = torch.min(dist_usv_to_unassigned, dim=1)
        feat_usv_opportunity = min_dist_usv.unsqueeze(1).expand(-1, T).clone()

    map_diag = torch.norm(map_size)
    if map_diag > 0:
        dist_ut /= map_diag
        feat_task_proximity /= map_diag
        feat_usv_opportunity /= map_diag

    usv_task_edges = torch.stack([dist_ut, feat_task_proximity, feat_usv_opportunity], dim=-1)
    return usv_task_edges

# æ–°çš„ä¸€å‘¨ä¿®æ”¹+åºå·2ï¼šå¢å¼ºç¯å¢ƒéªŒè¯å‡½æ•°
def validate_environment_integrity(env: USVEnv, agent: PPOAgent, device: torch.device, 
                                   validation_episodes: int = 3, save_path: Optional[str] = None):
    """
    é€šè¿‡å¤šæ¬¡è¿è¡Œå’Œç”˜ç‰¹å›¾å¯è§†åŒ–æ¥éªŒè¯ç¯å¢ƒçš„æ­£ç¡®æ€§
    
    Args:
        env: USVç¯å¢ƒå®ä¾‹
        agent: PPOæ™ºèƒ½ä½“
        device: è®¡ç®—è®¾å¤‡
        validation_episodes: éªŒè¯episodeæ•°é‡
        save_path: ç”˜ç‰¹å›¾ä¿å­˜è·¯å¾„
    
    Returns:
        validation_results: éªŒè¯ç»“æœå­—å…¸
    """
    print("\n" + "="*60)
    print("ğŸ” ENVIRONMENT INTEGRITY VALIDATION")
    print("="*60)
    
    validation_results = {
        'episodes_validated': 0,
        'successful_episodes': 0,
        'makespan_consistency': [],
        'assignment_errors': 0,
        'time_consistency_errors': 0,
        'gantt_charts_generated': 0
    }
    
    original_debug_mode = env.debug_mode
    env.set_debug_mode(True)  # å¯ç”¨è¯¦ç»†è°ƒè¯•ä¿¡æ¯
    
    for ep in range(validation_episodes):
        print(f"\nğŸ§ª Validation Episode {ep + 1}/{validation_episodes}")
        print("-" * 40)
        
        # é‡ç½®ç¯å¢ƒ
        state = env.reset()
        state['map_size'] = env.map_size
        
        # éªŒè¯åˆå§‹çŠ¶æ€
        if not env.validate_environment_state():
            print(f"âŒ Episode {ep + 1}: Initial state validation failed!")
            continue
            
        episode_steps = []
        done, step_count = False, 0
        max_steps = env.num_tasks * 2
        
        # è¿è¡Œä¸€ä¸ªå®Œæ•´çš„episode
        while not done and step_count < max_steps:
            # è®¡ç®—è¾¹ç‰¹å¾
            with torch.no_grad():
                usv_task_edges = compute_lookahead_features(state, device)
            
            # è·å–åŠ¨ä½œï¼ˆä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ä»¥ä¾¿å¤ç°ï¼‰
            action, _, _ = agent.get_action(state, usv_task_edges, deterministic=True)
            
            # è®°å½•æ­¥éª¤ä¿¡æ¯
            step_info = {
                'step': step_count,
                'action': action,
                'usv_idx': action // env.num_tasks,
                'task_idx': action % env.num_tasks,
                'prev_makespan': env.makespan
            }
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action)
            
            step_info.update({
                'new_makespan': env.makespan,
                'reward': reward,
                'done': done
            })
            episode_steps.append(step_info)
            
            # ä¸­é—´çŠ¶æ€éªŒè¯
            if not env.validate_environment_state():
                print(f"âŒ Episode {ep + 1}, Step {step_count}: State validation failed!")
                validation_results['assignment_errors'] += 1
                break
                
            state = next_state
            state['map_size'] = env.map_size
            step_count += 1
        
        # Episodeå®Œæˆåçš„éªŒè¯
        final_makespan = info.get('makespan', 0.0)
        calculated_makespan = max(u.available_time for u in env.usvs) if env.usvs else 0.0
        
        makespan_diff = abs(final_makespan - calculated_makespan)
        validation_results['makespan_consistency'].append(makespan_diff)
        
        if makespan_diff > 1e-6:
            print(f"âš ï¸  Episode {ep + 1}: Makespan inconsistency detected!")
            print(f"    Reported: {final_makespan:.6f}, Calculated: {calculated_makespan:.6f}")
            validation_results['time_consistency_errors'] += 1
        
        # ç”Ÿæˆç”˜ç‰¹å›¾è¿›è¡Œå¯è§†åŒ–éªŒè¯
        if save_path and (ep == 0 or makespan_diff > 1e-6):  # ç¬¬ä¸€ä¸ªepisodeæˆ–æœ‰é—®é¢˜çš„episode
            try:
                viz_manager = VisualizationManager(viz_name="validation_gantt", enabled=False)
                gantt_path = f"{save_path}_ep{ep + 1}_gantt.png"
                
                gantt_summary = viz_manager.generate_gantt_chart(env, save_path=gantt_path)
                validation_results['gantt_charts_generated'] += 1
                
                print(f"ğŸ“Š Gantt chart saved: {gantt_path}")
                print(f"    Final makespan: {gantt_summary['makespan']:.3f}")
                print(f"    Task assignments: {gantt_summary['total_tasks']} tasks assigned")
                print(f"    Load balance (Jain's): {gantt_summary['jains_index']:.4f}")
                
                # ä¿å­˜è¯¦ç»†çš„éªŒè¯æŠ¥å‘Š
                validation_report = {
                    'episode': ep + 1,
                    'steps_taken': step_count,
                    'final_makespan': final_makespan,
                    'calculated_makespan': calculated_makespan,
                    'makespan_difference': makespan_diff,
                    'gantt_summary': gantt_summary,
                    'episode_steps': episode_steps
                }
                
                report_path = f"{save_path}_ep{ep + 1}_report.json"
                with open(report_path, 'w', encoding='utf-8') as f:
                    json.dump(validation_report, f, indent=2, ensure_ascii=False)
                
            except Exception as e:
                print(f"âš ï¸  Failed to generate Gantt chart: {str(e)}")
        
        if done and step_count < max_steps:
            validation_results['successful_episodes'] += 1
            print(f"âœ… Episode {ep + 1}: Successfully completed in {step_count} steps")
        else:
            print(f"âŒ Episode {ep + 1}: Failed to complete properly")
        
        validation_results['episodes_validated'] += 1
    
    # æ¢å¤åŸå§‹è°ƒè¯•æ¨¡å¼
    env.set_debug_mode(original_debug_mode)
    
    # è¾“å‡ºéªŒè¯æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ VALIDATION SUMMARY")
    print("="*60)
    print(f"Episodes validated: {validation_results['episodes_validated']}")
    print(f"Successful episodes: {validation_results['successful_episodes']}")
    print(f"Success rate: {validation_results['successful_episodes']/validation_results['episodes_validated']*100:.1f}%")
    print(f"Assignment errors: {validation_results['assignment_errors']}")
    print(f"Time consistency errors: {validation_results['time_consistency_errors']}")
    print(f"Gantt charts generated: {validation_results['gantt_charts_generated']}")
    
    if validation_results['makespan_consistency']:
        avg_makespan_error = np.mean(validation_results['makespan_consistency'])
        max_makespan_error = np.max(validation_results['makespan_consistency'])
        print(f"Makespan consistency - Avg error: {avg_makespan_error:.2e}, Max error: {max_makespan_error:.2e}")
    
    # åˆ¤æ–­éªŒè¯æ˜¯å¦é€šè¿‡
    validation_passed = (
        validation_results['successful_episodes'] == validation_results['episodes_validated'] and
        validation_results['assignment_errors'] == 0 and
        validation_results['time_consistency_errors'] == 0
    )
    
    if validation_passed:
        print("ğŸ‰ ENVIRONMENT VALIDATION PASSED!")
    else:
        print("âŒ ENVIRONMENT VALIDATION FAILED!")
        print("   Please check the generated reports and Gantt charts for detailed analysis.")
    
    print("="*60 + "\n")
    
    return validation_results

# 8.21ä¿®æ”¹_åºå·10ï¼šå¢å¼ºçš„è¯„ä¼°é€»è¾‘ï¼ˆå¤šç‚¹é˜²æŠ¤ + ç»Ÿè®¡ä¿¡æ¯ï¼‰
# è¯´æ˜ï¼šåŠ å…¥æœ€å¤§æ­¥æ•°é˜²æŠ¤ã€makespanå¼‚å¸¸å…œåº•ã€ä¸´æ—¶å…³é—­envè°ƒè¯•è¾“å‡ºï¼Œè¿”å›æ ‡å‡†å·®/å¼‚å¸¸è®¡æ•°ï¼Œä¾¿äºç¨³å®šæ€§è¯„ä¼°ã€‚
def evaluate(env: USVEnv, agent: PPOAgent, episodes: int = 5, deterministic: bool = True):
    ms_list, rews_list, balance_list = [], [], []
    invalid_episodes = 0  # æ–°å¢ï¼šè®°å½•å¼‚å¸¸episodeæ•°é‡

    # ä¸´æ—¶ç¦ç”¨è°ƒè¯•æ¨¡å¼ä»¥å‡å°‘è¯„ä¼°è¾“å‡ºå™ªå£°
    original_debug = env.debug_mode
    env.set_debug_mode(False)

    for ep in range(episodes):
        state = env.reset()
        state['map_size'] = env.map_size
        done, total_r = False, 0.0
        step_count = 0
        max_steps = env.num_tasks * 2  # é˜²æ— é™å¾ªç¯

        while not done:
            with torch.no_grad():
                usv_task_edges = compute_lookahead_features(state, agent.device)
            a, _, _ = agent.get_action(state, usv_task_edges, deterministic=deterministic)
            state, r, done, info = env.step(a)
            state['map_size'] = env.map_size
            total_r += float(r)
            step_count += 1
            if step_count > max_steps:
                print(f"[WARN] Eval episode {ep} exceeded maximum steps, force stop")
                break

        makespan = info.get('makespan', 0.0)

        # makespanå¼‚å¸¸æ£€æµ‹ä¸å…œåº•
        min_task_time = min(t.processing_time for t in env.tasks) if env.tasks else 0
        if makespan <= 0 or makespan < min_task_time:
            print(f"[WARN] Invalid makespan in eval episode {ep}: {makespan}")
            invalid_episodes += 1
            makespan = max(min_task_time, 1.0)

        ms_list.append(makespan)
        rews_list.append(total_r)
        balance_metrics = env.get_balance_metrics()
        balance_list.append(balance_metrics['jains_index'])

    # æ¢å¤è°ƒè¯•æ¨¡å¼
    env.set_debug_mode(original_debug)

    if invalid_episodes > 0:
        print(f"[WARN] {invalid_episodes}/{episodes} evaluation episodes had invalid makespan")

    return {
        'makespan': float(np.mean(ms_list)),
        'reward': float(np.mean(rews_list)),
        'jains_index': float(np.mean(balance_list)),
        'makespan_std': float(np.std(ms_list)),
        'invalid_episodes': invalid_episodes
    }


class MetricsLogger:
    # 8.21ä¿®æ”¹_åºå·13ï¼šæ”¯æŒ"å°¾æ®µé€€ç«èµ·ç‚¹"å¯è§†åŒ–ï¼ˆtail_start_epï¼‰
    # è¯´æ˜ï¼šå¤šæ¥æ”¶ä¸€ä¸ªå¯é€‰å‚æ•°ï¼Œåœ¨æ‰€æœ‰å­å›¾ä¸Šç”»ç«–è™šçº¿å’Œæ·¡è‰²å°¾æ®µåŒºé—´ï¼Œä¾¿äºå®šä½é€€ç«å‰åæŒ‡æ ‡å˜åŒ–ã€‚
    def __init__(self, run_dir: Path, prefix: str, plot_every: int = 40, tail_start_ep: Optional[int] = None):
        self.run_dir = run_dir; self.prefix = prefix
        self.csv_path  = run_dir / f"{prefix}metrics.csv" 
        self.plot_path = run_dir / f"{prefix}metrics.png"
        self.plot_every = int(plot_every)
        self.tail_start_ep = tail_start_ep  # æ–°å¢ï¼šé€€ç«èµ·ç‚¹ï¼ˆepisodeï¼‰

        # 8.21ä¿®æ”¹_åºå·3ï¼šæ—¥å¿—åˆ—æ–°å¢ eval_makespan_emaï¼ˆç”¨äºç”»æ›´å¹³çš„è¯„ä¼°æ›²çº¿ï¼‰
        # è¯´æ˜ï¼šä¿ç•™åŸæœ‰åˆ—ï¼Œé¢å¤–è®°å½•"è¯„ä¼°å‡å€¼å†ç»EMAå¹³æ»‘"çš„makespanï¼Œå¸®åŠ©åˆ¤è¯»é•¿æœŸè¶‹åŠ¿ï¼›ä¸ä¼šå½±å“è®­ç»ƒä¸ä¿å­˜é€»è¾‘ã€‚
        self.headers = ["episode","train_makespan","train_reward","actor_loss","critic_loss","entropy_loss",
                        "eval_makespan","eval_makespan_ema","eval_reward","jains_index","task_load_variance","current_lr"]

        if not self.csv_path.exists():
            with open(self.csv_path, "w", newline="", encoding="utf-8") as f:
                csv.writer(f).writerow(self.headers)
        self.data = {h: [] for h in self.headers}

    def log(self, ep, metrics: dict):
        for h in self.headers: self.data[h].append(metrics.get(h, np.nan))
        with open(self.csv_path, "a", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([metrics.get(h) for h in self.headers])
        if ep % self.plot_every == 0 or ep == 1: self.plot()

    def plot(self):
        if not self.data["episode"]: return
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        axes = axes.flatten()

        plot_map = {
            'Makespan': ('train_makespan', 'eval_makespan'),
            'Reward': ('train_reward', 'eval_reward'),
            "Jain's Fairness Index": ('jains_index',),
            'Actor Loss': ('actor_loss',),
            'Critic Loss': ('critic_loss',),
            'Entropy Loss': ('entropy_loss',),
            'Task Load Variance': ('task_load_variance',),
            'Learning Rate': ('current_lr',)
        }

        for i, (title, keys) in enumerate(plot_map.items()):
            ax = axes[i]; ax.set_title(title); ax.set_xlabel("Episode")

            # 8.21ä¿®æ”¹_åºå·4ï¼šMakespanå­å›¾å¢åŠ ç¬¬ä¸‰æ¡EMAå¹³æ»‘è¯„ä¼°æ›²çº¿ï¼ˆä»…å±•ç¤ºï¼Œä¸å½±å“è®­ç»ƒï¼‰
            # è¯´æ˜ï¼ševal(EMA)æ¥è‡ªå¯¹è¯„ä¼°å‡å€¼åšEMA(é»˜è®¤alpha=0.1)ï¼Œè§†è§‰ä¸Šæ›´ç¨³å®šï¼Œä¾¿äºè§‚å¯ŸçœŸå®è¶‹åŠ¿ã€‚
            if title == 'Makespan':
                ax.plot(self.data['episode'], self.data['train_makespan'], label="train")
                if not all(np.isnan(self.data['eval_makespan'])):
                    ax.plot(self.data['episode'], self.data['eval_makespan'], linestyle="--", label="eval")
                if 'eval_makespan_ema' in self.data and not all(np.isnan(self.data['eval_makespan_ema'])):
                    ax.plot(self.data['episode'], self.data['eval_makespan_ema'], linestyle=":", label="eval(EMA)")
                ax.legend()
            else:
                ax.plot(self.data['episode'], self.data[keys[0]], label="train")
                if len(keys) > 1 and not all(np.isnan(self.data[keys[1]])):
                    ax.plot(self.data['episode'], self.data[keys[1]], linestyle="--", label="eval")
                if 'makespan' in keys[0] or 'reward' in keys[0]: ax.legend()

            # 8.21ä¿®æ”¹_åºå·15ï¼šåœ¨å­å›¾ä¸Šæ ‡è®°"å°¾æ®µé€€ç«èµ·ç‚¹"åŠå°¾æ®µåŒºé—´
            # è¯´æ˜ï¼šè™šçº¿è¡¨ç¤ºé€€ç«èµ·ç‚¹ï¼›å°¾æ®µåŒºé—´ç”¨æ·¡è‰²é®ç½©ï¼Œå¸®åŠ©å¯¹æ¯”é€€ç«å‰åæ›²çº¿å½¢æ€å˜åŒ–ã€‚
            if self.tail_start_ep is not None and len(self.data['episode']) > 0:
                ax.axvline(self.tail_start_ep, linestyle='--', alpha=0.6)
                last_ep = self.data['episode'][-1]
                if last_ep >= self.tail_start_ep:
                    ax.axvspan(self.tail_start_ep, last_ep, alpha=0.06)

            if 'Fairness' in title: ax.set_ylim(0, 1.05)
            if 'Learning Rate' in title: ax.set_yscale('log')
            ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.plot_path, dpi=150, bbox_inches='tight')
        plt.close(fig)  # ç¡®ä¿å…³é—­å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜


# 8.21ä¿®æ”¹_åºå·11ï¼šç¯å¢ƒçŠ¶æ€ç»“æ„å¿«é€Ÿæ£€æŸ¥ï¼ˆä¾¿äºæ’æŸ¥çŠ¶æ€å¼ é‡/æ©ç é—®é¢˜ï¼‰
# è¯´æ˜ï¼šåªåœ¨å¯åŠ¨æ—¶æ‰“å°ä¸€æ¬¡å…³é”®shapeä¸æ•°æ®èŒƒå›´ï¼Œå¸®åŠ©å®šä½æ½œåœ¨å¼‚å¸¸ï¼›ä¸å½±å“è®­ç»ƒæµç¨‹ã€‚
def _env_state_structure_check(env: USVEnv):
    print("\n=== Environment State Structure Check ===")
    sample_state = env.reset()
    print("State keys:", sample_state.keys())
    print("usv_features type/shape:",
          type(sample_state['usv_features']),
          sample_state['usv_features'].shape)
    print("task_features type/shape:",
          type(sample_state['task_features']),
          sample_state['task_features'].shape)
    print("action_mask type/shape:",
          type(sample_state['action_mask']),
          sample_state['action_mask'].shape)

    # é¢å¤–æ£€æŸ¥æ•°æ®èŒƒå›´
    print("\nData Range Check:")
    print("usv_features min/max:",
          np.min(sample_state['usv_features']),
          np.max(sample_state['usv_features']))
    print("task_features min/max:",
          np.min(sample_state['task_features']),
          np.max(sample_state['task_features']))
    print("action_mask sum:", np.sum(sample_state['action_mask']))

    # makespanåˆå§‹åŒ–æ£€æŸ¥
    print(f"Initial makespan: {env.makespan}")
    print("=======================================\n")


# 8.21ä¿®æ”¹_åºå·12ï¼šæ–°å¢"æ¨¡å‹æ¶æ„éªŒè¯"å‡½æ•°
# è¯´æ˜ï¼šåœ¨æ­£å¼è®­ç»ƒå‰åšä¸€æ¬¡HGNNä¸Agentçš„å‰å‘/å°ºå¯¸æ£€æŸ¥ï¼Œä¾¿äºå¿«é€Ÿå®šä½ç»´åº¦/è®¾å¤‡é—®é¢˜ã€‚
def validate_model_architecture(hgnn, agent, env, device):
    print("\n=== Model Architecture Validation ===")

    # è·å–æ ·æœ¬æ•°æ®
    sample_state = env.reset()
    sample_state['map_size'] = env.map_size

    # è½¬æ¢ä¸ºtorch tensors
    usv_features = torch.from_numpy(sample_state['usv_features']).float().unsqueeze(0).to(device)
    task_features = torch.from_numpy(sample_state['task_features']).float().unsqueeze(0).to(device)
    usv_task_edges = compute_lookahead_features(sample_state, device).unsqueeze(0)

    print(f"Input shapes:")
    print(f"  USV features: {usv_features.shape}")
    print(f"  Task features: {task_features.shape}")
    print(f"  USV-Task edges: {usv_task_edges.shape}")

    # æµ‹è¯•HGNNå‰å‘ä¼ æ’­
    try:
        with torch.no_grad():
            usv_emb, task_emb, graph_emb = hgnn(usv_features, task_features, usv_task_edges)
        print(f"âœ… HGNN forward pass successful!")
        print(f"  USV embeddings: {usv_emb.shape}")
        print(f"  Task embeddings: {task_emb.shape}")
        print(f"  Graph embeddings: {graph_emb.shape}")
    except Exception as e:
        print(f"âŒ HGNN forward pass failed: {e}")
        raise e

    # æµ‹è¯•Agentå‰å‘ä¼ æ’­
    try:
        with torch.no_grad():
            action, logp, value = agent.get_action(sample_state, usv_task_edges.squeeze(0), deterministic=False)
        print(f"âœ… Agent forward pass successful!")
        print(f"  Action: {action}")
        print(f"  Log probability: {logp}")
        print(f"  Value: {value}")
    except Exception as e:
        print(f"âŒ Agent forward pass failed: {e}")
        raise e

    # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in hgnn.parameters())
    trainable_params = sum(p.numel() for p in hgnn.parameters() if p.requires_grad)
    agent_params = sum(p.numel() for p in agent.parameters())

    print(f"ğŸ“Š Model Statistics:")
    print(f"  HGNN total parameters: {total_params:,}")
    print(f"  HGNN trainable parameters: {trainable_params:,}")
    print(f"  Agent total parameters: {agent_params:,}")
    print(f"  Memory usage: {torch.cuda.memory_allocated(device) / 1024**2:.1f} MB" if device.type == 'cuda' else "  Memory usage: CPU mode")

    print("=" * 50 + "\n")
    return True


# 8.21ä¿®æ”¹_åºå·5ï¼šæ–°å¢"ç­–ç•¥EMA"ç±»ï¼ˆä»…ç”¨äºè¯„ä¼°ä¸ä¿å­˜bestï¼Œä¸æ”¹å˜è®­ç»ƒå‚æ•°ï¼‰
# è¯´æ˜ï¼šç»å…¸çš„Polyak/EMAç¨³å®šå™¨ï¼›ç”¨æ»‘åŠ¨å¹³å‡æƒé‡è¿›è¡Œè¯„ä¼°/é€‰bestï¼Œå¯æ˜¾è‘—é™ä½ç­–ç•¥å¯¹åœºæ™¯æ‰°åŠ¨çš„æ•æ„Ÿåº¦ã€‚
class ParamEMA:
    def __init__(self, model, decay: float = 0.995):
        self.decay = float(decay)
        self.ema_state = {k: v.detach().clone().float() for k, v in model.state_dict().items()}

    @torch.no_grad()
    def update(self, model):
        cur = model.state_dict()
        for k, v in self.ema_state.items():
            v.mul_(self.decay).add_(cur[k].detach().float(), alpha=1.0 - self.decay)

    @torch.no_grad()
    def swap_in(self, model):
        """æŠŠEMAæƒé‡ä¸´æ—¶loadè¿›æ¥ï¼›è¿”å›åŸå§‹æƒé‡ç”¨äºæ¢å¤"""
        orig = {k: v.detach().clone() for k, v in model.state_dict().items()}
        model.load_state_dict(self.ema_state, strict=True)
        return orig

    @torch.no_grad()
    def swap_out(self, model, orig_state):
        model.load_state_dict(orig_state, strict=True)


def main():
    parser = argparse.ArgumentParser()
    # 8.21ä¿®æ”¹_åºå·6ï¼šé»˜è®¤é…ç½®è·¯å¾„ä¿æŒä¸ºimproved_config.jsonï¼ˆä¸ä½ æä¾›ä¸€è‡´ï¼Œå«æ–°å¢é”®ï¼‰
    # è¯´æ˜ï¼šç¡®ä¿æ–°åŠ çš„é”®ï¼ˆn_eval_episodes/use_ema_eval/ema_decay/eval_ema_alpha/plot_metrics_everyï¼‰èƒ½è¢«è¯»å–ã€‚
    parser.add_argument("--config", type=str,
                        default=os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "config", "improved_config.json"),
                        help="Path to config file")
    args = parser.parse_args()

    cfg = load_config(args.config)
    env_cfg, model_cfg, train_cfg = cfg['env_paras'], cfg['model_paras'], cfg['train_paras']
    device = _device_from_cfg(model_cfg)

    print(f"[INFO] Using device: {device}, Torch version: {torch.__version__}")
    print(f"[INFO] Model config: HGNN layers={model_cfg.get('num_hgnn_layers')}, dropout={model_cfg.get('dropout')}")
    print(f"[INFO] Train config: lr={train_cfg.get('lr')}, entropy_coeff={train_cfg.get('entropy_coeff')}")

    save_root = Path(train_cfg.get('save_root', "results/saved_models"))
    group_dir = save_root / _group_name(cfg)
    run_dir = group_dir / _next_run_index(group_dir)
    run_dir.mkdir(parents=True, exist_ok=True)
    prefix = f"{_short_ts()}_"
    print(f"[INFO] Artifacts will be saved in: {run_dir}")

    with open(run_dir / "config.json", "w") as f:
        json.dump(cfg, f, indent=2)

    env = USVEnv(env_cfg)

    # æ§åˆ¶è°ƒè¯•æ¨¡å¼
    debug_training = train_cfg.get('debug_mode', False)
    env.set_debug_mode(debug_training)
    if debug_training:
        print("[INFO] Debug mode enabled - detailed logs will be shown")

    # 8.21ä¿®æ”¹_åºå·11ï¼ˆè°ƒç”¨ï¼‰ï¼šå¯åŠ¨æ—¶è¿›è¡Œä¸€æ¬¡ç¯å¢ƒçŠ¶æ€ç»“æ„æ£€æŸ¥ï¼ˆä»…æ‰“å°ï¼Œä¸å½±å“è®­ç»ƒï¼‰
    _env_state_structure_check(env)

    # åˆ›å»ºæ¨¡å‹å¹¶éªŒè¯
    try:
        hgnn = HeterogeneousGNN(model_cfg).to(device)
        agent = PPOAgent(hgnn, model_cfg).to(device)
        print("âœ… Models created successfully")
    except Exception as e:
        print(f"âŒ Model creation failed: {e}")
        raise e

    # 8.21ä¿®æ”¹_åºå·12ï¼ˆè°ƒç”¨ï¼‰ï¼šè®­ç»ƒå‰åšä¸€æ¬¡æ¨¡å‹æ¶æ„éªŒè¯ï¼Œæå‰æš´éœ²æ½œåœ¨shape/è®¾å¤‡é—®é¢˜
    validate_model_architecture(hgnn, agent, env, device)

    # æ–°çš„ä¸€å‘¨ä¿®æ”¹+åºå·2ï¼šè¿›è¡Œç¯å¢ƒå®Œæ•´æ€§éªŒè¯
    # è¯´æ˜ï¼šåœ¨è®­ç»ƒå¼€å§‹å‰é€šè¿‡è¿è¡Œå¤šä¸ªéªŒè¯episodeå¹¶ç”Ÿæˆç”˜ç‰¹å›¾æ¥æ£€æµ‹ç¯å¢ƒçš„æ­£ç¡®æ€§
    validation_save_path = str(run_dir / f"{prefix}validation")
    env_validation_results = validate_environment_integrity(
        env, agent, device, 
        validation_episodes=train_cfg.get('env_validation_episodes', 3),
        save_path=validation_save_path
    )
    
    # å¦‚æœç¯å¢ƒéªŒè¯å¤±è´¥ï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
    if env_validation_results['successful_episodes'] < env_validation_results['episodes_validated']:
        print("âš ï¸  Environment validation detected issues. Check the generated reports.")
        print("    Training will continue, but results may be unreliable.")
        
        # ä¿å­˜éªŒè¯ç»“æœ
        with open(run_dir / f"{prefix}env_validation_results.json", 'w') as f:
            json.dump(env_validation_results, f, indent=2)

    ppo = PPO(agent, train_cfg)
    memory = Memory()

    viz = None
    if train_cfg.get('viz', False):
        try:
            viz = VisualizationManager(viz_name=train_cfg['viz_name'], enabled=True)
        except Exception as e:
            print(f"Warning: Failed to initialize Visdom. Live plotting disabled. Error: {e}")

    # 8.21ä¿®æ”¹_åºå·13ï¼ˆmainä¾§ï¼‰ï¼šè®¡ç®—"å°¾æ®µé€€ç«èµ·ç‚¹"episodeï¼ˆä¼˜å…ˆè¯»å–tail_start_episodeï¼Œå¦åˆ™æŒ‰æ¯”ä¾‹è®¡ç®—ï¼‰
    # è¯´æ˜ï¼šä¸ppo_policy.pyä¸­çš„é€€ç«é€»è¾‘å¯¹é½ï¼Œä¾¿äºåœ¨å›¾ä¸Šåšå¯è§†åŒ–æ ‡è®°ã€‚
    max_eps = int(train_cfg.get('max_episodes', 1000))
    tail_start_ep = int(train_cfg.get('tail_start_episode',
                        int(max_eps * float(train_cfg.get('tail_start_frac', 0.75)))))

    # 8.21ä¿®æ”¹_åºå·14ï¼šå°†å°¾æ®µèµ·ç‚¹ä¼ å…¥ MetricsLogger ä»¥ä¾¿ç”»è™šçº¿/é˜´å½±
    logger = MetricsLogger(run_dir, prefix,
                           plot_every=train_cfg.get('plot_metrics_every', 50),
                           tail_start_ep=tail_start_ep)

    best_eval_ms = float("inf")
    best_eval_reward = float("-inf")

    training_stats = {
        'episodes_completed': 0,
        'best_makespan': float('inf'),
        'best_reward': float('-inf'),
        'early_stop_triggered': False,
        'invalid_makespan_episodes': 0
    }

    # 8.21ä¿®æ”¹_åºå·7ï¼šåˆå§‹åŒ–"ç­–ç•¥EMA"ä¸è¯„ä¼°EMAç¼“å­˜ï¼ˆä»…ç”¨äºè¯„ä¼°/å±•ç¤ºï¼Œä¸å½±å“è®­ç»ƒï¼‰
    # è¯´æ˜ï¼šagent_emaç”¨äºswap_in/outè¯„ä¼°ï¼›eval_ms_emaç”¨äºç»˜å›¾ï¼ˆæ›´å¹³çš„evalçº¿ï¼‰ã€‚
    use_ema_eval = bool(train_cfg.get('use_ema_eval', True))
    ema_decay = float(train_cfg.get('ema_decay', 0.995))
    agent_ema = ParamEMA(agent, decay=ema_decay) if use_ema_eval else None

    eval_ema_alpha = float(train_cfg.get('eval_ema_alpha', 0.1))
    eval_ms_ema = None

    print(f"\nğŸš€ Starting training for {train_cfg['max_episodes']} episodes...")
    print("=" * 60)

    for ep in tqdm(range(1, train_cfg['max_episodes'] + 1), desc="Training Progress"):
        state = env.reset()
        state['map_size'] = env_cfg['map_size']
        done, ep_reward, last_value = False, 0.0, 0.0

        # episodeçº§åˆ«çš„å¼‚å¸¸æ£€æµ‹
        step_count = 0
        max_steps = env.num_tasks * 3  # è®¾ç½®æœ€å¤§æ­¥æ•°é™åˆ¶

        while not done and step_count < max_steps:
            with torch.no_grad():
                usv_task_edges = compute_lookahead_features(state, agent.device)

            action, logp, value = agent.get_action(state, usv_task_edges, deterministic=False)
            next_state, r, done, info = env.step(action)
            ep_reward += r
            step_count += 1

            memory.add(
                state,
                action,
                logp,
                float(r),
                bool(done),
                float(value),
                usv_task_edges.cpu()
            )
            state = next_state
            state['map_size'] = env_cfg['map_size']
            last_value = value

        # æ£€æŸ¥episodeæ˜¯å¦æ­£å¸¸å®Œæˆ
        makespan = info.get('makespan', 0.0)
        if makespan <= 0 or step_count >= max_steps:
            training_stats['invalid_makespan_episodes'] += 1
            if step_count >= max_steps:
                print(f"[WARN] Episode {ep} exceeded maximum steps ({max_steps})")
            if makespan <= 0:
                print(f"[WARN] Episode {ep} ended with invalid makespan: {makespan}")

        memory.values.append(last_value)
        losses = ppo.update(memory)
        memory.clear_memory()

        # 8.21ä¿®æ”¹_åºå·8ï¼šåœ¨æ¯ä¸ªepisodeæ›´æ–°åï¼Œåˆ·æ–°EMAå‚æ•°ç¼“å­˜
        # è¯´æ˜ï¼šä¿æŒEMAè¿½è¸ªåˆ°æœ€æ–°ç­–ç•¥ï¼›è®­ç»ƒä»ä½¿ç”¨åŸç­–ç•¥å‚æ•°ï¼ŒEMAä»…åœ¨è¯„ä¼°/ä¿å­˜bestæ—¶ä½¿ç”¨ã€‚
        if use_ema_eval:
            agent_ema.update(agent)

        log_metrics = {
            'episode': ep,
            'train_makespan': makespan,
            'train_reward': ep_reward,
            # â€”â€” æ–°å¢çš„ä¸¤ä¸ªå…¼å®¹é”®ï¼ˆå…³é”®ï¼‰â€”â€”
            'makespan': makespan,   # Visdom ç”¨
            'reward': ep_reward     # Visdom ç”¨
        }
        log_metrics.update(losses)
        log_metrics.update(env.get_balance_metrics())

        training_stats['episodes_completed'] = ep
        if makespan > 0 and makespan < training_stats['best_makespan']:
            training_stats['best_makespan'] = makespan
        if ep_reward > training_stats['best_reward']:
            training_stats['best_reward'] = ep_reward

        if viz and viz.enabled:
            viz.update_plots(ep, log_metrics)

        if ep % train_cfg['eval_frequency'] == 0:
            # 8.21ä¿®æ”¹_åºå·9ï¼šè¯„ä¼°é˜¶æ®µå¤šæ¬¡å–å‡å€¼ + ï¼ˆå¯é€‰ï¼‰ä½¿ç”¨EMAæƒé‡è¯„ä¼° + è®°å½•EMAå¹³æ»‘æŒ‡æ ‡
            # è¯´æ˜ï¼šn_eval_episodesé™ä½ç»Ÿè®¡å™ªå£°ï¼›EMAæƒé‡é™ä½ç­–ç•¥å¯¹åœºæ™¯çš„æ•æ„Ÿåº¦ï¼›eval_ms_emaç”¨äºç»˜å›¾æ›´ç¨³å®šã€‚
            n_eval = int(train_cfg.get('n_eval_episodes', 10))

            if use_ema_eval:
                _orig = agent_ema.swap_in(agent)  # EMAæƒé‡è£…å…¥agent
                eval_results = evaluate(env, agent, episodes=n_eval, deterministic=True)
                agent_ema.swap_out(agent, _orig)  # æ¢å¤è®­ç»ƒæƒé‡
            else:
                eval_results = evaluate(env, agent, episodes=n_eval, deterministic=True)

            cur_ms = float(eval_results['makespan'])
            eval_ms_ema = cur_ms if eval_ms_ema is None else (eval_ema_alpha*cur_ms + (1.0-eval_ema_alpha)*eval_ms_ema)

            log_metrics.update({
                'eval_makespan': eval_results['makespan'],
                'eval_makespan_ema': eval_ms_ema,
                'eval_reward': eval_results['reward']
            })

            print(f"\n[Eval Ep {ep:4d}] Makespan: {eval_results['makespan']:.2f}Â±{eval_results['makespan_std']:.2f}, "
                  f"Reward: {eval_results['reward']:.2f}, Jain's: {eval_results['jains_index']:.3f}")

            if eval_results.get('invalid_episodes', 0) > 0:
                print(f"  âš ï¸  {eval_results['invalid_episodes']}/{n_eval} eval episodes had issues")

            # ä¿å­˜bestï¼ˆè¯„ä¼°ç»“æœè‹¥æ¥è‡ªEMAç­–ç•¥ï¼Œå°†å¤©ç„¶æ›´ç¨³å®šï¼‰
            if eval_results['makespan'] > 0 and eval_results['makespan'] < best_eval_ms:
                best_eval_ms = eval_results['makespan']
                torch.save(agent.state_dict(), run_dir / f"{prefix}best_makespan_model.pt")
                print(f"  ğŸ¯ New best makespan model saved: {best_eval_ms:.2f}")

            if eval_results['reward'] > best_eval_reward:
                best_eval_reward = eval_results['reward']
                torch.save(agent.state_dict(), run_dir / f"{prefix}best_reward_model.pt")
                print(f"  ğŸ¯ New best reward model saved: {best_eval_reward:.2f}")

            # æ–°çš„ä¸€å‘¨ä¿®æ”¹+åºå·2ï¼šåœ¨ç‰¹å®šé—´éš”ç”Ÿæˆç”˜ç‰¹å›¾æ£€æŸ¥è®­ç»ƒè¿›å±•
            # è¯´æ˜ï¼šæ¯100ä¸ªepisodeæˆ–æœ€ä½³æ€§èƒ½æ›´æ–°æ—¶ç”Ÿæˆç”˜ç‰¹å›¾ï¼Œç¡®ä¿ç¯å¢ƒçŠ¶æ€å§‹ç»ˆæ­£ç¡®
            if (ep % 100 == 0 or eval_results['makespan'] < best_eval_ms * 1.01):
                try:
                    gantt_check_path = str(run_dir / f"{prefix}gantt_check_ep{ep}.png")
                    viz_check = VisualizationManager(viz_name="training_check", enabled=False)
                    
                    # ä¸´æ—¶å¼€å¯è°ƒè¯•æ¨¡å¼è¿›è¡Œè¯¦ç»†æ£€æŸ¥
                    env.set_debug_mode(True)
                    check_results = evaluate(env, agent, episodes=1, deterministic=True)
                    gantt_summary = viz_check.generate_gantt_chart(env, save_path=gantt_check_path)
                    env.set_debug_mode(debug_training)
                    
                    print(f"  ğŸ“Š Gantt check generated: ep{ep}, makespan={gantt_summary['makespan']:.2f}")
                    
                except Exception as e:
                    print(f"  âš ï¸  Failed to generate Gantt check: {e}")

            if ppo.check_early_stop(eval_results['reward']):
                print(f"\nâ„¹ï¸ Early stopping triggered at episode {ep}")
                training_stats['early_stop_triggered'] = True
                logger.log(ep, log_metrics)
                break

        logger.log(ep, log_metrics)

        if ep % train_cfg['save_frequency'] == 0:
            torch.save(agent.state_dict(), run_dir / f"{prefix}ep{ep:04d}.pt")
            torch.save(agent.state_dict(), run_dir / f"{prefix}latest.pt")

    print("\n" + "=" * 60)
    print("ğŸ‰ Training finished!")
    print(f"ğŸ“Š Episodes completed: {training_stats['episodes_completed']}")
    print(f"ğŸ“ˆ Best makespan achieved: {training_stats['best_makespan']:.2f}")
    print(f"ğŸ† Best reward achieved: {training_stats['best_reward']:.2f}")
    print(f"âš ï¸  Episodes with issues: {training_stats['invalid_makespan_episodes']}")
    if training_stats['early_stop_triggered']:
        print("â„¹ï¸ Training stopped early due to convergence")

    final_model_path = run_dir / f"{prefix}final_model.pt"
    torch.save(agent.state_dict(), final_model_path)

    with open(run_dir / f"{prefix}training_stats.json", "w") as f:
        serializable_stats = {k: float(v) if isinstance(v, np.number) else v
                              for k, v in training_stats.items()}
        json.dump(serializable_stats, f, indent=2)

    # æ–°çš„ä¸€å‘¨ä¿®æ”¹+åºå·2ï¼šæœ€ç»ˆç¯å¢ƒéªŒè¯å’Œè¯¦ç»†ç”˜ç‰¹å›¾ç”Ÿæˆ
    # è¯´æ˜ï¼šè®­ç»ƒå®Œæˆåè¿›è¡Œæœ€ç»ˆçš„ç¯å¢ƒéªŒè¯ï¼Œç”Ÿæˆè¯¦ç»†çš„ç”˜ç‰¹å›¾å’Œåˆ†ææŠ¥å‘Š
    try:
        print("\n" + "="*60)
        print("ğŸ FINAL VALIDATION AND REPORTING")
        print("="*60)
        
        viz_final = VisualizationManager(viz_name="final_gantt", enabled=False)

        best_model_path = run_dir / f"{prefix}best_makespan_model.pt"
        if best_model_path.exists():
            print(f"[INFO] Loading best makespan model for final analysis")
            agent.load_state_dict(torch.load(best_model_path, map_location=device))

        # æœ€ç»ˆç¯å¢ƒéªŒè¯ï¼ˆä½¿ç”¨æœ€ä½³æ¨¡å‹ï¼‰
        final_validation_path = str(run_dir / f"{prefix}final_validation")
        final_validation_results = validate_environment_integrity(
            env, agent, device, 
            validation_episodes=5,  # æ›´å¤šéªŒè¯episode
            save_path=final_validation_path
        )
        
        # ç”Ÿæˆæœ€ç»ˆç”˜ç‰¹å›¾
        env.set_debug_mode(True)
        evaluate(env, agent, episodes=1, deterministic=True)
        gantt_path = run_dir / f"{prefix}gantt_final.png"
        summary = viz_final.generate_gantt_chart(env, save_path=str(gantt_path))

        table_path = run_dir / f"{prefix}gantt_table.png"
        viz_final.save_summary_table(summary, env.makespan, str(table_path))
        print(f"âœ… Final Gantt chart and summary table saved.")

        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        final_balance = env.get_balance_metrics()
        print(f"ğŸ“Š Final Jain's Index: {final_balance['jains_index']:.4f}")
        print(f"ğŸ“Š Final Task Load Variance: {final_balance['task_load_variance']:.4f}")
        print(f"ğŸ“Š Final Makespan: {env.makespan:.2f}")
        
        # ä¿å­˜æœ€ç»ˆéªŒè¯æŠ¥å‘Š
        final_report = {
            'training_stats': training_stats,
            'final_validation': final_validation_results,
            'final_performance': {
                'makespan': env.makespan,
                'jains_index': final_balance['jains_index'],
                'task_load_variance': final_balance['task_load_variance']
            },
            'gantt_summary': summary
        }
        
        with open(run_dir / f"{prefix}final_report.json", 'w') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“‹ Final report saved: {run_dir / f'{prefix}final_report.json'}")

    except Exception as e:
        print(f"[WARN] Failed to generate final report: {e}")

    print(f"âœ… All artifacts saved in: {run_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()
