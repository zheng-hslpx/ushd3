from typing import Dict, Tuple
import torch
torch.set_default_dtype(torch.float32)
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import copy
import math

class Memory:
    def __init__(self):
        self.states, self.actions, self.logprobs, self.rewards, self.is_terminals, self.values = [],[],[],[],[],[]
        self.usv_features, self.task_features, self.action_masks = [],[],[]
        self.usv_task_edges = []

    def clear_memory(self):
        for lst in [self.states, self.actions, self.logprobs, self.rewards, self.is_terminals, 
                    self.values, self.usv_features, self.task_features, self.action_masks, self.usv_task_edges]:
            lst.clear()

    def add(self, state, action, logprob, reward, is_terminal, value, usv_task_edges):
        def _convert_and_validate(data, name):
            if isinstance(data, np.ndarray):
                tensor = torch.from_numpy(data)
            elif isinstance(data, torch.Tensor):
                tensor = data
            else:
                raise TypeError(f"{name} must be numpy array or torch tensor")
            
            if torch.isnan(tensor).any():
                raise ValueError(f"NaN values detected in {name}")
            if torch.isinf(tensor).any():
                raise ValueError(f"Inf values detected in {name}")
            return tensor

        try:
            usv_features = _convert_and_validate(state['usv_features'], 'usv_features')
            task_features = _convert_and_validate(state['task_features'], 'task_features')
            action_mask = _convert_and_validate(state['action_mask'], 'action_mask')
            
            self.states.append(state)
            self.actions.append(torch.tensor(action, dtype=torch.float32))
            self.logprobs.append(torch.tensor(logprob, dtype=torch.float32))
            self.rewards.append(float(reward))
            self.is_terminals.append(bool(is_terminal))
            self.values.append(float(value))
            self.usv_features.append(usv_features)
            self.task_features.append(task_features)
            self.action_masks.append(action_mask)
            self.usv_task_edges.append(usv_task_edges if isinstance(usv_task_edges, torch.Tensor) 
                                     else torch.from_numpy(usv_task_edges))
        except Exception as e:
            print(f"Error adding to memory: {str(e)}")
            print(f"State keys: {state.keys()}")
            if 'usv_features' in state:
                print(f"usv_features shape: {state['usv_features'].shape}")
            raise

class EnhancedActorNetwork(nn.Module):
    """*** å¢å¼ºç‰ˆActorç½‘ç»œ ***"""
    def __init__(self, input_dim: int, hidden_dim: int, num_hidden_layers: int, output_dim: int, dropout: float = 0.1):
        super().__init__()
        
        # *** æ”¹è¿›ï¼šæ›´æ·±çš„ç½‘ç»œæ¶æ„ ***
        layers = []
        dim = input_dim
        
        # è¾“å…¥æŠ•å½±å±‚
        layers.extend([
            nn.Linear(dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),  # ä½¿ç”¨GELUæ›¿ä»£Tanh
            nn.Dropout(dropout)
        ])
        dim = hidden_dim
        
        # ä¸»è¦éšè—å±‚
        for i in range(num_hidden_layers):
            layers.extend([
                nn.Linear(dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout)
            ])
            
            # *** æ–°å¢ï¼šæ®‹å·®è¿æ¥ï¼ˆæ¯ä¸¤å±‚ï¼‰ ***
            if i > 0 and i % 2 == 1:
                # æ·»åŠ æ®‹å·®è¿æ¥çš„æ ‡è®°ï¼Œåœ¨forwardä¸­å¤„ç†
                pass
        
        # è¾“å‡ºå±‚
        layers.extend([
            nn.Linear(dim, hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),  # è¾“å‡ºå±‚é™ä½dropout
            nn.Linear(hidden_dim // 2, output_dim)
        ])
        
        self.layers = nn.ModuleList()
        for layer in layers:
            self.layers.append(layer)
        
        # *** æ–°å¢ï¼šæ³¨æ„åŠ›æƒé‡ç”¨äºç‰¹å¾é‡è¦æ€§ ***
        self.feature_attention = nn.Sequential(
            nn.Linear(input_dim, input_dim // 4),
            nn.GELU(),
            nn.Linear(input_dim // 4, input_dim),
            nn.Sigmoid()
        )
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=math.sqrt(2))
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, usv_embeddings, task_embeddings, graph_embedding):
        B, U, E = usv_embeddings.shape
        T = task_embeddings.shape[1]
        
        # *** æ”¹è¿›ï¼šç‰¹å¾ç»„åˆå’Œæ³¨æ„åŠ›åŠ æƒ ***
        usv_exp = usv_embeddings.unsqueeze(2).expand(B, U, T, E)
        task_exp = task_embeddings.unsqueeze(1).expand(B, U, T, E)
        graph_exp = graph_embedding.unsqueeze(1).unsqueeze(1).expand(B, U, T, graph_embedding.shape[-1])
        
        # æ‹¼æ¥ç‰¹å¾
        feat = torch.cat([usv_exp, task_exp, graph_exp], dim=-1).view(B, U*T, -1)
        
        # *** æ–°å¢ï¼šç‰¹å¾æ³¨æ„åŠ›åŠ æƒ ***
        attn_weights = self.feature_attention(feat)
        feat = feat * attn_weights
        
        # *** æ”¹è¿›ï¼šé€šè¿‡ç½‘ç»œä¼ æ’­ ***
        x = feat
        residual = None
        
        for i, layer in enumerate(self.layers):
            if isinstance(layer, nn.Linear) and i > 8 and i % 8 == 0 and residual is not None:
                # æ¯8å±‚æ·»åŠ ä¸€ä¸ªæ®‹å·®è¿æ¥
                x = layer(x) + residual
                residual = x
            else:
                x = layer(x)
                if isinstance(layer, nn.Dropout) and i < 8:
                    residual = x
        
        return x.squeeze(-1)

class EnhancedCriticNetwork(nn.Module):
    """*** æ ¸å¿ƒæ”¹è¿›ï¼šæ›´å¼ºçš„ä»·å€¼ç½‘ç»œ ***"""
    def __init__(self, input_dim: int, hidden_dim: int, num_hidden_layers: int, dropout: float = 0.1):
        super().__init__()
        
        # *** å…³é”®æ”¹è¿›ï¼šå¢åŠ ç½‘ç»œæ·±åº¦å’Œå®½åº¦ ***
        enhanced_hidden_dim = hidden_dim * 2  # å¢åŠ å®½åº¦
        enhanced_layers = max(num_hidden_layers + 2, 4)  # å¢åŠ æ·±åº¦
        
        layers = []
        dim = input_dim
        
        # *** æ–°å¢ï¼šå¤šå°ºåº¦ç‰¹å¾æå– ***
        self.multi_scale_proj = nn.ModuleList([
            nn.Linear(input_dim, enhanced_hidden_dim),
            nn.Linear(input_dim, enhanced_hidden_dim // 2),
            nn.Linear(input_dim, enhanced_hidden_dim // 4)
        ])
        
        # ä¸»ç½‘ç»œ
        layers.extend([
            nn.Linear(enhanced_hidden_dim + enhanced_hidden_dim//2 + enhanced_hidden_dim//4, enhanced_hidden_dim),
            nn.LayerNorm(enhanced_hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        ])
        
        # *** æ”¹è¿›ï¼šæ›´æ·±çš„éšè—å±‚ ***
        dim = enhanced_hidden_dim
        for i in range(enhanced_layers):
            layers.extend([
                nn.Linear(dim, enhanced_hidden_dim),
                nn.LayerNorm(enhanced_hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout * (0.8 ** i))  # é€å±‚é€’å‡dropout
            ])
            
            # *** æ–°å¢ï¼šæ¯éš”ä¸¤å±‚æ·»åŠ æ®‹å·®è¿æ¥ ***
            if i > 0 and i % 2 == 1:
                layers.append(ResidualBlock(enhanced_hidden_dim, dropout))
        
        # *** æ”¹è¿›ï¼šå¤šå±‚è¾“å‡ºå¤´ ***
        layers.extend([
            nn.Linear(enhanced_hidden_dim, enhanced_hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(enhanced_hidden_dim // 2, enhanced_hidden_dim // 4),
            nn.GELU(),
            nn.Linear(enhanced_hidden_dim // 4, 1)
        ])
        
        self.net = nn.Sequential(*layers)
        
        # *** æ–°å¢ï¼šä»·å€¼èŒƒå›´é¢„æµ‹è¾…åŠ©å¤´ ***
        self.value_range_head = nn.Sequential(
            nn.Linear(input_dim, enhanced_hidden_dim // 2),
            nn.GELU(),
            nn.Linear(enhanced_hidden_dim // 2, 2)  # é¢„æµ‹ä»·å€¼çš„ä¸Šä¸‹ç•Œ
        )
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=math.sqrt(2))
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, graph_embedding):
        # *** æ–°å¢ï¼šå¤šå°ºåº¦ç‰¹å¾æå– ***
        multi_scale_feats = []
        for proj in self.multi_scale_proj:
            multi_scale_feats.append(F.gelu(proj(graph_embedding)))
        
        # ç»„åˆå¤šå°ºåº¦ç‰¹å¾
        combined_feat = torch.cat(multi_scale_feats, dim=-1)
        
        # ä¸»ä»·å€¼é¢„æµ‹
        value = self.net(combined_feat)
        
        # *** æ–°å¢ï¼šä»·å€¼èŒƒå›´è¾…åŠ©ç›‘ç£ ***
        value_range = self.value_range_head(graph_embedding)
        value_min, value_max = value_range[:, 0:1], value_range[:, 1:2]
        
        # ç¡®ä¿ä»·å€¼åœ¨åˆç†èŒƒå›´å†…
        value = torch.clamp(value, value_min, value_max)
        
        return value

class ResidualBlock(nn.Module):
    """*** æ–°å¢ï¼šæ®‹å·®å— ***"""
    def __init__(self, hidden_dim: int, dropout: float = 0.1):
        super().__init__()
        self.block = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )
        
    def forward(self, x):
        return F.gelu(x + self.block(x))

class AdaptiveLearningRateScheduler:
    """*** æ–°å¢ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦å™¨ ***"""
    def __init__(self, optimizer, initial_lr: float = 1e-4, patience: int = 50, factor: float = 0.8):
        self.optimizer = optimizer
        self.initial_lr = initial_lr
        self.patience = patience
        self.factor = factor
        
        self.best_critic_loss = float('inf')
        self.wait = 0
        self.warmup_steps = 100
        self.current_step = 0
        
    def step(self, critic_loss: float, reward_trend: float):
        self.current_step += 1
        
        # å‰æœŸwarmup
        if self.current_step <= self.warmup_steps:
            lr = self.initial_lr * (self.current_step / self.warmup_steps)
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr
            return
        
        # åŸºäºæ€§èƒ½çš„è‡ªé€‚åº”è°ƒæ•´
        if critic_loss < self.best_critic_loss * 0.99:  # æ˜¾è‘—æ”¹å–„
            self.best_critic_loss = critic_loss
            self.wait = 0
            # å°å¹…æå‡å­¦ä¹ ç‡
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = min(param_group['lr'] * 1.05, self.initial_lr * 2)
        else:
            self.wait += 1
            if self.wait >= self.patience:
                # é™ä½å­¦ä¹ ç‡
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] *= self.factor
                self.wait = 0
                print(f"[INFO] Learning rate reduced to {self.optimizer.param_groups[0]['lr']:.2e}")

# =============================================================================
# *** æ¢ç´¢æœºåˆ¶æ ¸å¿ƒå®ç°å¼€å§‹ - EXPLORATION MECHANISM CORE IMPLEMENTATION ***
# =============================================================================

class EnhancedPPOAgent(nn.Module):
    """*** å¢å¼ºç‰ˆPPOæ™ºèƒ½ä½“ - é›†æˆepsilon-greedyéšæœºæ¢ç´¢æœºåˆ¶ ***"""
    def __init__(self, hgnn_model: nn.Module, config: dict):
        super().__init__()
        E = int(config['embedding_dim'])
        dropout = float(config.get('dropout', 0.1))
        
        self.hgnn = hgnn_model
        
        # *** æ ¸å¿ƒæ”¹è¿›ï¼šæ›´å¼ºçš„ç½‘ç»œæ¶æ„ ***
        actor_hidden = int(config.get('n_latent_actor', 128)) * 2  # å¢åŠ Actorå®¹é‡
        critic_hidden = int(config.get('n_latent_critic', 128)) * 2  # å¢åŠ Criticå®¹é‡
        
        self.actor = EnhancedActorNetwork(
            4*E, actor_hidden, 
            int(config.get('n_hidden_actor', 2)) + 1,  # å¢åŠ å±‚æ•°
            1, dropout
        )
        self.critic = EnhancedCriticNetwork(
            2*E, critic_hidden, 
            int(config.get('n_hidden_critic', 2)) + 2,  # æ˜¾è‘—å¢åŠ Criticå±‚æ•°
            dropout
        )

        # =======================================================================
        # *** æ¢ç´¢æœºåˆ¶å‚æ•°é…ç½® - EXPLORATION MECHANISM CONFIGURATION ***
        # =======================================================================
        self.exploration_config = {
            'initial_epsilon': config.get('initial_epsilon', 0.3),    # åˆå§‹æ¢ç´¢ç‡30%
            'min_epsilon': config.get('min_epsilon', 0.05),          # æœ€å°æ¢ç´¢ç‡5%
            'epsilon_decay': config.get('epsilon_decay', 0.995),     # æ¢ç´¢ç‡è¡°å‡ç³»æ•°
            'exploration_steps': config.get('exploration_steps', 1000),  # æ¢ç´¢é˜¶æ®µæ­¥æ•°é˜ˆå€¼
            'adaptive_epsilon': config.get('adaptive_epsilon', True), # å¯ç”¨è‡ªé€‚åº”æ¢ç´¢ç‡è°ƒæ•´
        }
        
        # æ¢ç´¢æœºåˆ¶è¿è¡Œæ—¶çŠ¶æ€
        self.current_epsilon = self.exploration_config['initial_epsilon']
        self.training_step = 0
        self.train_flag = True  # è®­ç»ƒæ¨¡å¼æ ‡å¿—ï¼šTrue=è®­ç»ƒæ¨¡å¼(å¯ç”¨æ¢ç´¢), False=è¯„ä¼°æ¨¡å¼(ç¦ç”¨æ¢ç´¢)
        self.exploration_stats = {
            'total_random_actions': 0,      # æ€»éšæœºåŠ¨ä½œæ•°
            'recent_random_actions': 0,     # æœ€è¿‘çš„éšæœºåŠ¨ä½œæ•°
            'exploration_episodes': 0,      # æ¢ç´¢çš„å›åˆæ•°
        }

        # ç½‘ç»œåˆå§‹åŒ–æ£€æŸ¥
        print("\n=== Enhanced Network Initialization Check ===")
        actor_params = sum(p.numel() for p in self.actor.parameters())
        critic_params = sum(p.numel() for p in self.critic.parameters())
        print(f"Actor parameters: {actor_params:,}")
        print(f"Critic parameters: {critic_params:,}")
        print(f"Total parameters: {(actor_params + critic_params):,}")
        print("=" * 50)
        print(f"ğŸ¯ EXPLORATION MECHANISM INITIALIZED")
        print(f"   Initial Epsilon: {self.current_epsilon:.3f}")
        print(f"   Min Epsilon: {self.exploration_config['min_epsilon']:.3f}")
        print(f"   Decay Rate: {self.exploration_config['epsilon_decay']:.4f}")
        print(f"   Exploration Steps Threshold: {self.exploration_config['exploration_steps']}")
        print("=" * 50)
        
        # *** æ–°å¢ï¼šæ¢¯åº¦ç›‘æ§ ***
        self.grad_stats = {'actor': [], 'critic': []}
        
        self.old_hgnn = copy.deepcopy(hgnn_model)
        self.old_actor = copy.deepcopy(self.actor)
        self.old_critic = copy.deepcopy(self.critic)
        
        for p in self.old_hgnn.parameters(): p.requires_grad = False
        for p in self.old_actor.parameters(): p.requires_grad = False
        for p in self.old_critic.parameters(): p.requires_grad = False
        
        self.device = next(self.hgnn.parameters()).device

    def _to_dev(self, x, dtype=torch.float32):
        if not isinstance(x, torch.Tensor):
            x = torch.from_numpy(np.array(x))
        return x.to(self.device, dtype=dtype)

    # ==========================================================================
    # *** æ¢ç´¢æœºåˆ¶æ ¸å¿ƒæ–¹æ³• - EXPLORATION MECHANISM CORE METHODS ***
    # ==========================================================================
    
    def update_epsilon(self):
        """*** æ¢ç´¢ç‡åŠ¨æ€æ›´æ–°æœºåˆ¶ - Dynamic Epsilon Update Mechanism ***"""
        self.training_step += 1
        old_epsilon = self.current_epsilon
        
        # åˆ†é˜¶æ®µè¡°å‡ç­–ç•¥ï¼šå‰æœŸå¿«é€Ÿè¡°å‡ï¼ŒåæœŸç¼“æ…¢è¡°å‡
        if self.training_step < self.exploration_config['exploration_steps']:
            # å‰æœŸï¼šæŒ‡æ•°è¡°å‡æ¢ç´¢ç‡
            self.current_epsilon = max(
                self.exploration_config['min_epsilon'],
                self.current_epsilon * self.exploration_config['epsilon_decay']
            )
        else:
            # åæœŸï¼šçº¿æ€§è¡°å‡åˆ°æœ€å°å€¼ï¼Œä¿æŒå°‘é‡æ¢ç´¢
            decay_ratio = min(1.0, (self.training_step - self.exploration_config['exploration_steps']) / 2000)
            self.current_epsilon = max(
                self.exploration_config['min_epsilon'],
                self.exploration_config['min_epsilon'] * 2 * (1 - decay_ratio)
            )
        
        # è¾“å‡ºæ¢ç´¢ç‡å˜åŒ–æ—¥å¿—
        if abs(old_epsilon - self.current_epsilon) > 0.001:
            print(f"ğŸ”„ [EXPLORATION UPDATE] Step {self.training_step}: Îµ {old_epsilon:.4f} â†’ {self.current_epsilon:.4f}")
    
    def set_train_mode(self, train_flag: bool):
        """*** è®­ç»ƒ/è¯„ä¼°æ¨¡å¼åˆ‡æ¢ - Training/Evaluation Mode Switch ***"""
        self.train_flag = train_flag
        if not train_flag:
            print(f"ğŸ“Š [MODE SWITCH] â†’ EVALUATION Mode (epsilon = 0, deterministic actions)")
        else:
            print(f"ğŸ¯ [MODE SWITCH] â†’ TRAINING Mode (epsilon = {self.current_epsilon:.3f}, exploration enabled)")

    def get_exploration_stats(self):
        """*** è·å–æ¢ç´¢ç»Ÿè®¡ä¿¡æ¯ - Get Exploration Statistics ***"""
        return {
            'current_epsilon': self.current_epsilon,
            'training_step': self.training_step,
            'train_mode': self.train_flag,
            'exploration_phase': 'early' if self.training_step < self.exploration_config['exploration_steps'] else 'late',
            'total_random_actions': self.exploration_stats['total_random_actions'],
            'recent_random_actions': self.exploration_stats['recent_random_actions'],
        }

    # ==========================================================================
    # *** åŠ¨ä½œé€‰æ‹©æ ¸å¿ƒæ–¹æ³•ï¼ˆå«æ¢ç´¢æœºåˆ¶ï¼‰ - Action Selection with Exploration ***
    # ==========================================================================
    
    @torch.no_grad()
    def get_action(self, state: Dict, usv_task_edges: torch.Tensor, deterministic: bool=False) -> Tuple[int, float, float]:
        """*** é›†æˆæ¢ç´¢æœºåˆ¶çš„åŠ¨ä½œé€‰æ‹©æ–¹æ³• - Action Selection with Exploration Mechanism ***"""
        uf = self._to_dev(state['usv_features']).unsqueeze(0)
        tf = self._to_dev(state['task_features']).unsqueeze(0)
        am = self._to_dev(state['action_mask'], dtype=torch.bool).unsqueeze(0)
        ute = usv_task_edges.unsqueeze(0)

        # é€šè¿‡HGNNè·å–èŠ‚ç‚¹åµŒå…¥å’Œå›¾åµŒå…¥
        ue, te, ge = self.old_hgnn(uf, tf, ute)
        scores = self.old_actor(ue, te, ge)
        
        # *** åŠ¨ä½œæ©ç åº”ç”¨ - Apply Action Mask ***
        masked_scores = scores.masked_fill(~am, -1e8)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆåŠ¨ä½œ
        valid_actions = am.squeeze(0)
        if torch.all(~valid_actions):
            # æ— æœ‰æ•ˆåŠ¨ä½œæ—¶è¿”å›é»˜è®¤åŠ¨ä½œ
            action, logp = torch.tensor(0, device=self.device), torch.tensor(0.0, device=self.device)
            value = self.old_critic(ge).squeeze(-1)
            return int(action.item()), float(logp.item()), float(value.item())
        
        # =======================================================================
        # *** æ¢ç´¢æœºåˆ¶å†³ç­–æ ¸å¿ƒé€»è¾‘ - EXPLORATION MECHANISM DECISION LOGIC ***
        # =======================================================================
        
        # åªåœ¨è®­ç»ƒæ¨¡å¼ä¸”éç¡®å®šæ€§é€‰æ‹©æ—¶å¯ç”¨æ¢ç´¢
        use_exploration = self.train_flag and not deterministic
        
        if use_exploration:
            epsilon = self.current_epsilon
            
            # ğŸ² epsilon-greedyæ¢ç´¢ç­–ç•¥å†³ç­–ç‚¹
            if torch.rand(1).item() < epsilon:
                # *** æ‰§è¡Œéšæœºæ¢ç´¢åŠ¨ä½œ - EXECUTE RANDOM EXPLORATION ACTION ***
                valid_indices = torch.where(valid_actions)[0]
                if len(valid_indices) > 0:
                    # ä»æœ‰æ•ˆåŠ¨ä½œä¸­éšæœºé€‰æ‹©
                    random_idx = torch.randint(0, len(valid_indices), (1,), device=self.device)
                    action = valid_indices[random_idx]
                    
                    # è®¡ç®—éšæœºåŠ¨ä½œçš„logæ¦‚ç‡ï¼ˆç”¨äºç­–ç•¥æ¢¯åº¦è®­ç»ƒï¼‰
                    temperature = 1.0
                    probs = F.softmax(masked_scores / temperature, dim=-1)
                    dist = torch.distributions.Categorical(probs=probs)
                    logp = dist.log_prob(action)
                    
                    # ç»Ÿè®¡éšæœºåŠ¨ä½œä¿¡æ¯
                    usv_idx = action.item() // tf.shape[1]
                    task_idx = action.item() % tf.shape[1]
                    self.exploration_stats['total_random_actions'] += 1
                    self.exploration_stats['recent_random_actions'] += 1
                    
                    # ğŸ¯ æ¢ç´¢åŠ¨ä½œæ—¥å¿—è¾“å‡º
                    print(f"ğŸ² [RANDOM EXPLORATION] USV-{usv_idx} â†’ Task-{task_idx} "
                          f"(Îµ={epsilon:.3f}, total_random={self.exploration_stats['total_random_actions']})")
                    
                    # *** æ— äººèˆ¹ä»»åŠ¡è§„åˆ’ç‰¹å®šçš„æ¢ç´¢åˆç†æ€§æ£€æŸ¥ ***
                    # æ£€æŸ¥å½“å‰USVçš„è´Ÿè½½æƒ…å†µ
                    current_usv_assignments = state.get('usv_assignments', {}).get(str(usv_idx), [])
                    if len(current_usv_assignments) > 3:  # å¦‚æœUSVå·²æœ‰è¾ƒå¤šä»»åŠ¡
                        print(f"âš ï¸  [EXPLORATION WARNING] USV-{usv_idx} has high load: {len(current_usv_assignments)} tasks")
                    
                else:
                    # æ— æœ‰æ•ˆåŠ¨ä½œæ—¶çš„å›é€€å¤„ç†
                    action = torch.tensor(0, device=self.device)
                    logp = torch.tensor(0.0, device=self.device)
            else:
                # *** æ‰§è¡Œç­–ç•¥ç½‘ç»œåŠ¨ä½œ - EXECUTE POLICY NETWORK ACTION ***
                # ä½¿ç”¨ç­–ç•¥ç½‘ç»œè¿›è¡ŒåŠ¨ä½œé€‰æ‹©ï¼ˆè´ªå¿ƒæˆ–é‡‡æ ·ï¼‰
                temperature = 0.1 if deterministic else 1.0
                probs = F.softmax(masked_scores / temperature, dim=-1)
                dist = torch.distributions.Categorical(probs=probs)
                
                if deterministic:
                    action = torch.argmax(probs, dim=-1)
                else:
                    action = dist.sample()
                
                logp = dist.log_prob(action)
                
                # ç­–ç•¥åŠ¨ä½œæ—¥å¿—ï¼ˆè¾ƒå°‘è¾“å‡ºï¼‰
                if torch.rand(1).item() < 0.1:  # 10%æ¦‚ç‡è¾“å‡ºç­–ç•¥åŠ¨ä½œæ—¥å¿—
                    usv_idx = action.item() // tf.shape[1]
                    task_idx = action.item() % tf.shape[1]
                    print(f"ğŸ§  [POLICY ACTION] USV-{usv_idx} â†’ Task-{task_idx} (policy-guided)")
        else:
            # *** è¯„ä¼°æ¨¡å¼æˆ–ç¡®å®šæ€§æ¨¡å¼ï¼šçº¯ç­–ç•¥é€‰æ‹© ***
            temperature = 0.1 if deterministic else 1.0
            probs = F.softmax(masked_scores / temperature, dim=-1)
            
            dist = torch.distributions.Categorical(probs=probs)
            if deterministic:
                action = torch.argmax(probs, dim=-1)
                logp = dist.log_prob(action)
            else:
                action = dist.sample()
                logp = dist.log_prob(action)
        
        # ä»·å€¼å‡½æ•°è¯„ä¼°
        value = self.old_critic(ge).squeeze(-1)
        
        # *** æœ€ç»ˆåŠ¨ä½œéªŒè¯ - Final Action Validation ***
        final_usv_idx = action.item() // tf.shape[1]
        final_task_idx = action.item() % tf.shape[1]
        
        # ç¡®ä¿åŠ¨ä½œåœ¨æœ‰æ•ˆèŒƒå›´å†…
        if not valid_actions[action.item()]:
            print(f"âŒ [ERROR] Selected invalid action: USV-{final_usv_idx} â†’ Task-{final_task_idx}")
            # å›é€€åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆåŠ¨ä½œ
            valid_indices = torch.where(valid_actions)[0]
            if len(valid_indices) > 0:
                action = valid_indices[0]
                logp = dist.log_prob(action)
        
        return int(action.item()), float(logp.item()), float(value.item())
    
    # =======================================================================
    # *** æ¢ç´¢æœºåˆ¶è¾…åŠ©æ–¹æ³• - EXPLORATION MECHANISM AUXILIARY METHODS ***
    # =======================================================================
    
    def reset_exploration_episode_stats(self):
        """*** é‡ç½®å›åˆæ¢ç´¢ç»Ÿè®¡ - Reset Episode Exploration Statistics ***"""
        self.exploration_stats['recent_random_actions'] = 0
        self.exploration_stats['exploration_episodes'] += 1
    
    def log_exploration_summary(self):
        """*** è¾“å‡ºæ¢ç´¢æœºåˆ¶æ€»ç»“ä¿¡æ¯ - Log Exploration Summary ***"""
        stats = self.get_exploration_stats()
        print("\n" + "="*60)
        print("ğŸ¯ EXPLORATION MECHANISM SUMMARY")
        print("="*60)
        print(f"Current Epsilon: {stats['current_epsilon']:.4f}")
        print(f"Training Step: {stats['training_step']}")
        print(f"Mode: {'TRAINING' if stats['train_mode'] else 'EVALUATION'}")
        print(f"Phase: {stats['exploration_phase'].upper()}")
        print(f"Total Random Actions: {stats['total_random_actions']}")
        print(f"Recent Random Actions: {stats['recent_random_actions']}")
        print(f"Exploration Episodes: {self.exploration_stats['exploration_episodes']}")
        print("="*60 + "\n")

# =============================================================================
# *** æ¢ç´¢æœºåˆ¶æ ¸å¿ƒå®ç°ç»“æŸ - EXPLORATION MECHANISM CORE IMPLEMENTATION END ***
# =============================================================================

    def evaluate_actions(self, uf_b, tf_b, am_b, ute_b, actions_b):
        ue, te, ge = self.hgnn(uf_b, tf_b, ute_b)
        scores = self.actor(ue, te, ge)
        
        am_b_bool = am_b.to(torch.bool)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ ·æœ¬å®Œå…¨æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œ
        no_valid_actions = torch.all(~am_b_bool, dim=1)
        if torch.any(no_valid_actions):
            print(f"Warning: {no_valid_actions.sum().item()} samples have no valid actions!")
            am_b_bool[no_valid_actions, 0] = True
        
        # *** æ”¹è¿›ï¼šæ›´ç¨³å®šçš„æ¦‚ç‡è®¡ç®— ***
        masked_scores = scores.masked_fill(~am_b_bool, -1e8)
        
        # æ•°å€¼ç¨³å®šæ€§å¤„ç†
        max_scores = torch.max(masked_scores, dim=-1, keepdim=True).values
        stable_scores = masked_scores - max_scores
        
        # è®¡ç®—æ¦‚ç‡
        exp_scores = torch.exp(stable_scores) * am_b_bool.float()
        probs_sum = exp_scores.sum(dim=-1, keepdim=True)
        probs = exp_scores / (probs_sum + 1e-20)
        
        # æœ€ç»ˆåˆæ³•æ€§æ£€æŸ¥
        probs = torch.clamp(probs, min=1e-10, max=1.0)
        probs = probs / probs.sum(dim=-1, keepdim=True)
        
        dist = torch.distributions.Categorical(probs=probs)
        return dist.log_prob(actions_b), self.critic(ge).squeeze(-1), dist.entropy()

    def update_old_policy(self):
        self.old_hgnn.load_state_dict(self.hgnn.state_dict())
        self.old_actor.load_state_dict(self.actor.state_dict())
        self.old_critic.load_state_dict(self.critic.state_dict())
    
    def get_gradient_stats(self):
        """*** æ–°å¢ï¼šæ¢¯åº¦ç»Ÿè®¡ ***"""
        actor_grad_norm = 0
        critic_grad_norm = 0
        
        for p in self.actor.parameters():
            if p.grad is not None:
                actor_grad_norm += p.grad.data.norm(2).item() ** 2
        
        for p in self.critic.parameters():
            if p.grad is not None:
                critic_grad_norm += p.grad.data.norm(2).item() ** 2
        
        actor_grad_norm = actor_grad_norm ** 0.5
        critic_grad_norm = critic_grad_norm ** 0.5
        
        self.grad_stats['actor'].append(actor_grad_norm)
        self.grad_stats['critic'].append(critic_grad_norm)
        
        return actor_grad_norm, critic_grad_norm

class EnhancedPPO:
    """*** å¢å¼ºç‰ˆPPOè®­ç»ƒå™¨ - æ”¯æŒæ¢ç´¢æœºåˆ¶ ***"""
    def __init__(self, agent: EnhancedPPOAgent, config: dict):
        self.agent = agent
        
        # *** æ”¹è¿›ï¼šæ›´ç²¾ç»†çš„è¶…å‚æ•° ***
        self.gamma = float(config.get('gamma', 0.99))
        self.eps_clip = float(config.get('eps_clip', 0.15))  # ç•¥å¾®å¢åŠ 
        self.K_epochs = int(config.get('K_epochs', 4))  # å‡å°‘epochæ•°
        self.vf_coeff = float(config.get('vf_coeff', 0.3))  # é™ä½value lossæƒé‡
        self.entropy_coeff = float(config.get('entropy_coeff', 0.02))  # å¢åŠ æ¢ç´¢
        self.minibatch_size = int(config.get('minibatch_size', 64))  # å‡å°batch size
        self.gae_lambda = float(config.get('gae_lambda', 0.95))
        self.max_grad_norm = float(config.get('max_grad_norm', 0.5))
        
        # *** æ ¸å¿ƒæ”¹è¿›ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ ***
        initial_lr = float(config.get('lr', 8e-5))  # é™ä½åˆå§‹å­¦ä¹ ç‡
        self.optimizer = torch.optim.AdamW(  # ä½¿ç”¨AdamW
            self.agent.parameters(), 
            lr=initial_lr, 
            eps=1e-5, 
            weight_decay=1e-4  # æ·»åŠ æƒé‡è¡°å‡
        )
        
        self.scheduler = AdaptiveLearningRateScheduler(self.optimizer, initial_lr)
        
        # æ—©åœå‚æ•°
        self.early_stop_patience = int(config.get('early_stop_patience', 150))
        self.best_eval_reward = float('-inf')
        self.patience_counter = 0
        
        # *** æ–°å¢ï¼šè®­ç»ƒç»Ÿè®¡å’Œæ¢ç´¢ç®¡ç† ***
        self.training_stats = {
            'critic_losses': [],
            'actor_losses': [],
            'gradient_norms': {'actor': [], 'critic': []},
            'exploration_stats': []  # æ–°å¢æ¢ç´¢ç»Ÿè®¡
        }
        
        # =======================================================================
        # *** æ¢ç´¢æœºåˆ¶è®­ç»ƒè°ƒåº¦é…ç½® - EXPLORATION TRAINING SCHEDULE CONFIG ***
        # =======================================================================
        self.exploration_schedule = {
            'update_frequency': config.get('epsilon_update_frequency', 10),        # æ¯10æ¬¡æ›´æ–°è°ƒæ•´ä¸€æ¬¡epsilon
            'performance_threshold': config.get('exploration_performance_threshold', -50),  # æ€§èƒ½é˜ˆå€¼
            'adaptive_exploration': config.get('adaptive_exploration', True),       # è‡ªé€‚åº”æ¢ç´¢å¼€å…³
            'exploration_boost_episodes': config.get('exploration_boost_episodes', 100),  # å‰100å›åˆåŠ å¼ºæ¢ç´¢
        }
        
        self.update_count = 0
        
        print("ğŸ¯ PPO TRAINING WITH EXPLORATION MECHANISM INITIALIZED")
        print(f"   Update frequency: every {self.exploration_schedule['update_frequency']} updates")
        print(f"   Performance threshold: {self.exploration_schedule['performance_threshold']}")
        print(f"   Adaptive exploration: {self.exploration_schedule['adaptive_exploration']}")

    def update(self, memory: Memory, eval_reward: float = None):
        dev = self.agent.device
        self.update_count += 1
        
        # =======================================================================
        # *** æ¢ç´¢æœºåˆ¶æ›´æ–°è°ƒåº¦ - EXPLORATION MECHANISM UPDATE SCHEDULE ***
        # =======================================================================
        if self.update_count % self.exploration_schedule['update_frequency'] == 0:
            old_epsilon = self.agent.current_epsilon
            
            # åŸºäºæ€§èƒ½çš„è‡ªé€‚åº”æ¢ç´¢ç‡è°ƒæ•´
            if self.exploration_schedule['adaptive_exploration'] and eval_reward is not None:
                if eval_reward < self.exploration_schedule['performance_threshold']:
                    # æ€§èƒ½ä¸ä½³ï¼Œé€‚åº¦å¢åŠ æ¢ç´¢
                    self.agent.current_epsilon = min(
                        self.agent.current_epsilon * 1.1,
                        self.agent.exploration_config['initial_epsilon'] * 0.8
                    )
                    print(f"ğŸ“ˆ [ADAPTIVE EXPLORATION] Performance low ({eval_reward:.1f}), "
                          f"boosted Îµ: {old_epsilon:.3f} â†’ {self.agent.current_epsilon:.3f}")
                else:
                    # æ€§èƒ½è‰¯å¥½ï¼Œæ­£å¸¸è¡°å‡æ¢ç´¢ç‡
                    self.agent.update_epsilon()
                    
                    if abs(old_epsilon - self.agent.current_epsilon) > 0.001:
                        print(f"ğŸ“‰ [NORMAL DECAY] Performance good ({eval_reward:.1f}), "
                              f"Îµ: {old_epsilon:.3f} â†’ {self.agent.current_epsilon:.3f}")
            else:
                # æ ‡å‡†æ¢ç´¢ç‡æ›´æ–°
                self.agent.update_epsilon()
        
        # å­¦ä¹ ç‡ç›‘æ§
        current_lr = self.optimizer.param_groups[0]['lr']
        print(f"\nğŸ”§ Update #{self.update_count} - LR: {current_lr:.2e}, Epsilon: {self.agent.current_epsilon:.3f}")
        
        rewards = torch.tensor(memory.rewards, dtype=torch.float32, device=dev)
        values = torch.tensor(memory.values, dtype=torch.float32, device=dev)
        terminals = torch.tensor(memory.is_terminals, dtype=torch.float32, device=dev)
        
        # å¥–åŠ±ç»Ÿè®¡
        print(f"Rewards - min: {rewards.min().item():.2f}, max: {rewards.max().item():.2f}, mean: {rewards.mean().item():.2f}")
        
        # *** æ”¹è¿›ï¼šæ›´ç²¾ç¡®çš„ä¼˜åŠ¿ä¼°è®¡ ***
        advantages = torch.zeros_like(rewards)
        last_gae_lam = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                nextnonterminal = 1.0 - terminals[t]
                nextvalue = values[t]
            else:
                nextnonterminal = 1.0 - terminals[t]
                nextvalue = values[t + 1]
            
            delta = rewards[t] + self.gamma * nextvalue * nextnonterminal - values[t]
            advantages[t] = last_gae_lam = delta + self.gamma * self.gae_lambda * nextnonterminal * last_gae_lam
        
        returns = advantages + values[:-1]
        
        # *** æ”¹è¿›ï¼šæ›´ç¨³å®šçš„ä¼˜åŠ¿å½’ä¸€åŒ– ***
        if len(advantages) > 1 and advantages.std() > 1e-8:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # å‡†å¤‡æ‰¹é‡æ•°æ®
        old_uf = self.to_tensor(memory.usv_features, dev)
        old_tf = self.to_tensor(memory.task_features, dev)
        old_am = self.to_tensor(memory.action_masks, dev)
        old_ute = self.to_tensor(memory.usv_task_edges, dev)
        old_actions = self.to_tensor(memory.actions, dev, torch.long)
        old_logprobs = self.to_tensor(memory.logprobs, dev)
        
        total_actor_loss, total_critic_loss, total_entropy_loss = 0, 0, 0
        num_updates = 0
        
        for epoch in range(self.K_epochs):
            indices = torch.randperm(len(old_actions), device=dev)
            
            for start_idx in range(0, len(old_actions), self.minibatch_size):
                end_idx = min(start_idx + self.minibatch_size, len(old_actions))
                batch_indices = indices[start_idx:end_idx]
                
                logprobs, vals, entropy = self.agent.evaluate_actions(
                    old_uf[batch_indices], old_tf[batch_indices], 
                    old_am[batch_indices], old_ute[batch_indices], 
                    old_actions[batch_indices]
                )
                
                # *** æ”¹è¿›ï¼šæ›´ç¨³å®šçš„ratioè®¡ç®— ***
                ratios = torch.exp(torch.clamp(logprobs - old_logprobs[batch_indices], -10, 10))
                surr1 = ratios * advantages[batch_indices]
                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages[batch_indices]
                
                actor_loss = -torch.min(surr1, surr2).mean()
                
                # *** æ”¹è¿›ï¼šæ›´é²æ£’çš„ä»·å€¼æŸå¤± ***
                value_target = returns[batch_indices]
                value_loss_unclipped = F.mse_loss(vals, value_target)
                
                # ä»·å€¼è£å‰ªï¼ˆå¯é€‰ï¼‰
                vals_clipped = old_logprobs[batch_indices] + torch.clamp(
                    vals - old_logprobs[batch_indices], -self.eps_clip, self.eps_clip
                )
                value_loss_clipped = F.mse_loss(vals_clipped, value_target)
                critic_loss = torch.max(value_loss_unclipped, value_loss_clipped)
                
                entropy_loss = entropy.mean()
                
                # *** æ–°å¢ï¼šæ¢ç´¢é˜¶æ®µè‡ªé€‚åº”æŸå¤±æƒé‡ ***
                exploration_factor = 1.0 + 0.5 * self.agent.current_epsilon  # æ¢ç´¢æœŸå¢åŠ ç†µæƒé‡
                
                total_loss = (actor_loss + 
                             self.vf_coeff * critic_loss - 
                             self.entropy_coeff * exploration_factor * entropy_loss)
                
                self.optimizer.zero_grad()
                total_loss.backward()
                
                # *** æ”¹è¿›ï¼šè‡ªé€‚åº”æ¢¯åº¦è£å‰ª ***
                actor_grad_norm, critic_grad_norm = self.agent.get_gradient_stats()
                
                # æ ¹æ®æ¢¯åº¦å†å²åŠ¨æ€è°ƒæ•´è£å‰ªé˜ˆå€¼
                if len(self.training_stats['gradient_norms']['critic']) > 10:
                    recent_critic_grads = self.training_stats['gradient_norms']['critic'][-10:]
                    adaptive_clip = min(self.max_grad_norm * 2, np.percentile(recent_critic_grads, 75))
                else:
                    adaptive_clip = self.max_grad_norm
                
                total_grad_norm = torch.nn.utils.clip_grad_norm_(self.agent.parameters(), adaptive_clip)
                
                self.optimizer.step()
                
                # ç»Ÿè®¡æ›´æ–°
                total_actor_loss += actor_loss.item()
                total_critic_loss += critic_loss.item()
                total_entropy_loss += entropy_loss.item()
                num_updates += 1
        
        self.agent.update_old_policy()
        
        # *** æ ¸å¿ƒæ”¹è¿›ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦ ***
        avg_critic_loss = total_critic_loss / num_updates
        avg_reward = rewards.mean().item()
        self.scheduler.step(avg_critic_loss, avg_reward)
        
        # *** æ–°å¢ï¼šæ›´æ–°è®­ç»ƒç»Ÿè®¡ï¼ˆåŒ…æ‹¬æ¢ç´¢ä¿¡æ¯ï¼‰ ***
        self.training_stats['critic_losses'].append(avg_critic_loss)
        self.training_stats['actor_losses'].append(total_actor_loss / num_updates)
        self.training_stats['exploration_stats'].append(self.agent.get_exploration_stats())
        
        return {
            'actor_loss': total_actor_loss / num_updates,
            'critic_loss': avg_critic_loss,
            'entropy_loss': total_entropy_loss / num_updates,
            'current_lr': self.optimizer.param_groups[0]['lr'],
            'grad_norm_actor': np.mean(self.agent.grad_stats['actor'][-10:]) if self.agent.grad_stats['actor'] else 0,
            'grad_norm_critic': np.mean(self.agent.grad_stats['critic'][-10:]) if self.agent.grad_stats['critic'] else 0,
            # *** æ¢ç´¢æœºåˆ¶ç›¸å…³ç»Ÿè®¡ä¿¡æ¯ ***
            'exploration_epsilon': self.agent.current_epsilon,
            'exploration_phase': self.training_stats['exploration_stats'][-1]['exploration_phase'],
            'total_random_actions': self.training_stats['exploration_stats'][-1]['total_random_actions'],
        }

    def check_early_stop(self, eval_reward: float) -> bool:
        if eval_reward > self.best_eval_reward:
            self.best_eval_reward = eval_reward
            self.patience_counter = 0
            return False
        else:
            self.patience_counter += 1
            return self.patience_counter >= self.early_stop_patience

    def to_tensor(self, data, device, dtype=None):
        return torch.tensor(np.array(data), device=device, dtype=dtype)

# å…¼å®¹æ€§åˆ«å
PPOAgent = EnhancedPPOAgent
PPO = EnhancedPPO